// Copyright (c) 2016-2017 Matthias Noack <ma.noack.pr@gmail.com>
//
// See accompanying file LICENSE and README for further information.

#ifndef noma_num_rk_stepper_hpp
#define noma_num_rk_stepper_hpp

#include <cassert>

#include <noma/ocl/helper.hpp>
#include <noma/ocl/kernel_wrapper.hpp>

#include "noma/num/butcher_tableau.hpp"

namespace noma {
namespace num {

/**
 * List of modi for accumulating the final result and the k's during a
 * Runge-Kutta step.
 * - separated: Always uses the weighted_add kernel.
 * - integrated: Accumulate the final result of all weighted k's during
 *   ODE-evaluation instead of touching all the k's again in the end. This
 *   costs one additional ODE state buffer.
 * - subdiagonal: Assumes a subdiagonal structure for the butcher tableau
 *   (classic RK4). Needs only two temporary buffers and no weighted add
 *   kernel calls at all.
 */
enum class accumulate_method {
	separated,
	integrated,
	subdiagonal // implies integrated
};


template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD = accumulate_method::separated>
class rk_stepper : public ocl::kernel_wrapper
{
public:
	using ode_type = ODE_T;

	static constexpr accumulate_method acc_method = ACC_METHOD;

	rk_stepper(ocl::helper& ocl, const std::string& source_header, const std::string& ocl_compile_options, const ocl::nd_range& range, ODE_T& ode);
	rk_stepper(ocl::helper& ocl, const boost::filesystem::path& rk_weighted_add_file_name, const std::string& rk_weighted_add_kernel_name,
	           const std::string& source_header, const std::string& ocl_compile_options, const ocl::nd_range& range, ODE_T& ode);

	real_t step(real_t time, real_t step_size, cl::Buffer& d_mem_in, cl::Buffer& d_mem_out);

	// generate OpenCL compile options
	static void ode_compile_options(std::ostream& os); // NOTE: needs to be static, as this is needed for ODE construction, which happens before stepper construction

private:
	void initialise();
	void set_dynamic_args(real_t step_size, cl::Buffer& d_mem_in, cl::Buffer& d_mem_out, const std::vector<double>& coeffs);

	// method specification
	const butcher_tableau b_tab;

	ODE_T& ode;

	// OpenCL buffers
	std::vector<cl::Buffer> k_buffers;
	cl::Buffer tmp_buffer; // for integrated accumulation

	// constants derived from the OpenCL implementation
	// NOTE: must be consistent with number of buffer arguments in rk_weighted_add OpenCL kernel
	const size_t max_buffers_in_kernel = 7;
	const size_t first_buffer_kernel_arg = 4;

	static const std::string embedded_ocl_source_;
	static const std::string embedded_ocl_kernel_name_;
};

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
const std::string rk_stepper<ODE_T, RKM, ACC_METHOD>::embedded_ocl_source_ {
#include "rk_weighted_add.cl.hpp"  // NOTE: generated by CMake
};
template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
const std::string rk_stepper<ODE_T, RKM, ACC_METHOD>::embedded_ocl_kernel_name_ { "rk_weighted_add" };

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
rk_stepper<ODE_T, RKM, ACC_METHOD>::rk_stepper(ocl::helper& ocl, const std::string& source_header, const std::string& ocl_compile_options, const ocl::nd_range& range, ODE_T& ode)
	: ocl::kernel_wrapper(ocl, embedded_ocl_source_, embedded_ocl_kernel_name_, source_header, ocl_compile_options, range), b_tab(get_butcher_tableau(RKM)), ode(ode)
{
	initialise();
}

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
rk_stepper<ODE_T, RKM, ACC_METHOD>::rk_stepper(ocl::helper& ocl, const boost::filesystem::path& rk_weighted_add_file_name, const std::string& rk_weighted_add_kernel_name,
                                               const std::string& source_header, const std::string& ocl_compile_options, const ocl::nd_range& range, ODE_T& ode)
	: ocl::kernel_wrapper(ocl, rk_weighted_add_file_name, rk_weighted_add_kernel_name, source_header, ocl_compile_options, range), b_tab(get_butcher_tableau(RKM)), ode(ode)
{
	initialise();
}

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
void rk_stepper<ODE_T, RKM, ACC_METHOD>::initialise()
{
	// create buffers for k_1 to k_n
	size_t num_buffs = b_tab.a.size(); // default: one buffer per row in butcher tableau's a matrix

	if (ACC_METHOD == accumulate_method::subdiagonal)
		num_buffs = 2; // only two buffers needed if butcher tableau has subdiagonal structure

	for (size_t i = 0; i < num_buffs; ++i)
		k_buffers.push_back(ocl_.create_buffer(CL_MEM_READ_WRITE, ode.buffer_size_byte(), nullptr));

	// one additional buffer for integrated accumulation, since the weighted add for the next ode evaluation and the final result are needed at the same time
	if (ACC_METHOD == accumulate_method::integrated)
		tmp_buffer = ocl_.create_buffer(CL_MEM_READ_WRITE, ode.buffer_size_byte(), nullptr);
};

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
void rk_stepper<ODE_T, RKM, ACC_METHOD>::ode_compile_options(std::ostream& os)
{
	if (acc_method == accumulate_method::integrated ||
	    acc_method == accumulate_method::subdiagonal) {
		os << "#define NOMA_NUM_ODE_ACCUMULATE" << "\n";
	}

	if (acc_method == accumulate_method::subdiagonal) {
		os<< "#define NOMA_NUM_SUBDIAGONAL" << "\n";
	}
}

template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
void rk_stepper<ODE_T, RKM, ACC_METHOD>::set_dynamic_args(real_t step_size, cl::Buffer& d_mem_in, cl::Buffer& d_mem_out, const std::vector<double>& coeffs)
{
	cl_int err = 0;
	err = kernel_.setArg(0, static_cast<int>(coeffs.size()));
	ocl::error_handler(err, "clSetKernelArg(0)");
	err = kernel_.setArg(1, step_size);
	ocl::error_handler(err, "clSetKernelArg(1)");
	err = kernel_.setArg(2, d_mem_out);
	ocl::error_handler(err, "clSetKernelArg(2)");
	err = kernel_.setArg(3, d_mem_in);
	ocl::error_handler(err, "clSetKernelArg(3)");

	assert(coeffs.size() <= max_buffers_in_kernel);

	size_t offset = first_buffer_kernel_arg;
	for (size_t i = 0; i < coeffs.size(); ++i)
	{
		// set coefficient
		err = kernel_.setArg(2*i + offset, coeffs[i]);
		ocl::error_handler(err, "kernel_.setArg(2*i + offset, coeffs[i])");
		// set buffer
		err = kernel_.setArg(2*i + offset + 1, k_buffers[i]);
		ocl::error_handler(err, "kernel_.setArg(2*i + offset + 1, k_buffers[i])");
	}
	// make sure the unsused buffers are also set, otherweise OpenCL (at least Intel's implementation) segfaults
	offset += coeffs.size()*2;
	for (size_t i = 0; i < (max_buffers_in_kernel - coeffs.size()); ++i)
	{
		double null_coeff = 0.0;
		err = kernel_.setArg(2*i + offset, null_coeff);
		ocl::error_handler(err, "kernel_.setArg(2*i + offset, null_coeff)");
		err = kernel_.setArg(2*i + offset + 1, k_buffers[0]);
		ocl::error_handler(err, "kernel_.setArg(2*i + offset + 1, k_buffers[0])");
	}
}

/* performs a single integration step */
template<typename ODE_T, rk_method_t RKM, accumulate_method ACC_METHOD>
real_t rk_stepper<ODE_T, RKM, ACC_METHOD>::step(real_t time, real_t step_size, cl::Buffer& d_mem_in, cl::Buffer& d_mem_out)
{
	if (ACC_METHOD == accumulate_method::separated) {
		// compute k1
		// h = step_size
		// k1 = f(t_n, y_n), t_n not relevant, implicit via y_n = y(t_n)
		// reads from d_mem_in, writes mathematical k1
		ode.solve(time, step_size * b_tab.c[0], d_mem_in, k_buffers[0], b_tab.b[0] * step_size);

		// compute k_2 to k_n
		for (size_t i = 1; i < b_tab.a.size(); ++i) {
			// weighted sum:
			// reads from d_mem_in, writes_to d_mem_out
			set_dynamic_args(step_size, d_mem_in, d_mem_out, b_tab.a[i]); // write sum into tmp
			run_kernel();

			// reads from d_mem_out, where the weighted sum of the ks is, and writes the next k_n
			ode.solve(time, step_size * b_tab.c[i], d_mem_out, k_buffers[i], b_tab.b[0] * step_size);
		}

		// compute results
		set_dynamic_args(step_size, d_mem_in, d_mem_out, b_tab.b); // TODO(adaptive time step): loop over b, or hardcode for 2 possible results
		// y_n+1 = y_n + 1/6 k1 + 1/3 k2 + 1/3 k3 + 1/6 k4
		run_kernel(); // call wrapped weighted add kernel
	} else if (ACC_METHOD == accumulate_method::integrated) {
		// compute k1
		// h = step_size
		// k1 = f(t_n, y_n), t_n not relevant, implicit via y_n = y(t_n)
		// reads from d_mem_in, writes mathematical k1, initialises d_mem_out and accumulates weighted (b_tab.b[0]) math. k1 into it
		// NOTE:
		// we compute k_i =  f(t_n + c_i * h, y_n + a_(i+1, i) * h)
		// and: y_(n+1) = y_n + sum(i) ( b_i * h * k_i)
		// t_n is time, h is step_size
		// the ODE time_step is for the current runge-kutta sub-step is: b_tab.c[0] * step_size
		// b_i * h is the acc_coeff = b_tab.c[0] * step_size
		ode.solve(time, b_tab.c[0] * step_size, d_mem_in, k_buffers[0], d_mem_out, b_tab.b[0] * step_size, true); // compute and accumulate

		// compute and accumulate k_2 to k_n
		for (size_t i = 1; i <  b_tab.a.size(); ++i)
		{
			// weighted sum of needed k's as input for next k_i = f(..., THIS_IS_COMPUTED):
			// reads from d_mem_in, writes_to tmp_buffer
			set_dynamic_args(step_size, d_mem_in, tmp_buffer, b_tab.a[i]); // write sum into tmp
			run_kernel();

			// reads from tmp_buffer, where the weighted sum of the ks is, and writes the next k_n, also adds weighted (b_tab.b[i]) k_n to d_mem_out
			ode.solve(time, b_tab.c[i] * step_size, tmp_buffer, k_buffers[i], d_mem_out, b_tab.b[i] * step_size, false);
		}
		// NOTE: no final weighted add needed, for the cost of having the addition tmp_buffer, should save memory bandwidth though
	} else if (ACC_METHOD == accumulate_method::subdiagonal) {
		// compute k1
		// h = step_size
		// k1 = f(t_n, y_n), t_n not relevant, implicit via y_n = y(t_n)
		// reads from d_mem_in, writes mathematical k1, initialises and d_mem_out, and accumulates weighted k0
		ode.solve(time, b_tab.c[0] * step_size, d_mem_in, k_buffers[0], b_tab.a[1][0] * step_size, d_mem_out, b_tab.b[0] * step_size, true); // compute and accumulate

		// compute and accumulate k_2 to k_n
		size_t i = 1;
		auto get_in_buf  = [&]() -> cl::Buffer& { return (i % 2) == 1 ? k_buffers[0] : k_buffers[1]; };
		auto get_out_buf = [&]() -> cl::Buffer& { return (i % 2) == 1 ? k_buffers[1] : k_buffers[0]; };
		for (; i <  (b_tab.a.size() - 1); ++i)
		{
			// reads from tmp_buffer, where the weighted sum of the ks is, and writes the next k_n, also adds weighted k_n to d_mem_out
			ode.solve(time, b_tab.c[i] * step_size, get_in_buf(), get_out_buf(), b_tab.a[i+1][i] * step_size, d_mem_out, b_tab.b[i] * step_size, false);
		}
		ode.solve(time, b_tab.c[i] * step_size,  get_in_buf(), get_out_buf(), d_mem_out, b_tab.b[i] * step_size, false);
	}

	// compute comparison results if b_cmp is set
	// decide if comparison is done here, or an optional second output argument is used, such that the solver can use it's norm for comparison
//	if (!b_tab.b_cmp.empty())
//	{
//		set_dynamic_args(step_size, d_mem_in, /*TODO*/, b_tab.b_cmp); // TODO(adaptive time step): loop over b, or hardcode for 2 possible results
//		run_kernel(); // call wrapped weighted add kernel
//	}

	// TODO(adaptive time step): return some error metric, or maybe current step_size, depending on where the error norm is evaluated and the time step is set
	return 0.0;
}

} // namespace num
} // namespace noma

#endif // noma_num_rk_stepper_hpp
